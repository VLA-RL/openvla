{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 22:36:48.032423: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-08 22:36:48.032540: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-08 22:36:48.090694: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-08 22:36:48.199996: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-08 22:36:49.594747: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-08 22:36:52.604417: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 22:36:52.609890: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 22:36:52.610107: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import draccus\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import tqdm\n",
    "import wandb\n",
    "from accelerate import PartialState\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "from prismatic.models.backbones.llm.prompting import PurePromptBuilder, VicunaV15ChatPromptBuilder\n",
    "from prismatic.util.data_utils import PaddedCollatorForActionPrediction\n",
    "from prismatic.vla.action_tokenizer import ActionTokenizer\n",
    "from prismatic.vla.datasets import RLDSBatchTransform, RLDSDataset\n",
    "from prismatic.vla.datasets.rlds.utils.data_utils import save_dataset_statistics\n",
    "\n",
    "# Sane Defaults\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FinetuneConfig:\n",
    "    # fmt: off\n",
    "    vla_path: str = \"/media/lawrence/Work/checkpoints/openvla-7b\"   # Path to OpenVLA model \n",
    "\n",
    "    # Directory Paths\n",
    "    data_root_dir: Path = Path(\"./datasets\")        # Path to Open-X dataset directory\n",
    "    dataset_name: str = \"imperialcollege_sawyer_wrist_cam\"                                # Name of fine-tuning dataset (e.g., `droid_wipe`)\n",
    "    run_root_dir: Path = Path(\"./runs\")                               # Path to directory to store logs & checkpoints\n",
    "    adapter_tmp_dir: Path = Path(\"./adapter-tmp\")                     # Temporary directory for LoRA weights before fusing\n",
    "\n",
    "    # Fine-tuning Parameters\n",
    "    batch_size: int = 2#16                                            # Fine-tuning batch size\n",
    "    max_steps: int = 200#200_000                                        # Max number of fine-tuning steps\n",
    "    save_steps: int = 5#5000                                          # Interval for checkpoint saving\n",
    "    learning_rate: float = 2e-5                                     # Fine-tuning learning rate\n",
    "    grad_accumulation_steps: int = 1                                # Gradient accumulation steps\n",
    "    image_aug: bool = True                                          # Whether to train with image augmentations\n",
    "    shuffle_buffer_size: int = 100#100_000                              # Dataloader shuffle buffer size (can reduce if OOM)\n",
    "\n",
    "    # LoRA Arguments\n",
    "    use_lora: bool = True                                           # Whether to use LoRA fine-tuning\n",
    "    lora_rank: int = 32                                             # Rank of LoRA weight matrix\n",
    "    lora_dropout: float = 0.0                                       # Dropout applied to LoRA weights\n",
    "    use_quantization: bool = True                                  # Whether to 4-bit quantize VLA for LoRA fine-tuning\n",
    "                                                                    #   => CAUTION: Reduces memory but hurts performance\n",
    "\n",
    "    # Tracking Parameters\n",
    "    wandb_project: str = \"openvla\"                                  # Name of W&B project to log to (use default!)\n",
    "    wandb_entity: str = \"lawrence-rs-lin\"                          # Name of entity to log under\n",
    "\n",
    "    # fmt: on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning OpenVLA Model `/media/lawrence/Work/checkpoints/openvla-7b` on `imperialcollege_sawyer_wrist_cam`\n"
     ]
    }
   ],
   "source": [
    "cfg = FinetuneConfig\n",
    "print(f\"Fine-tuning OpenVLA Model `{cfg.vla_path}` on `{cfg.dataset_name}`\")\n",
    "\n",
    "# [Validate] Ensure GPU Available & Set Device / Distributed Context\n",
    "assert torch.cuda.is_available(), \"Fine-tuning assumes at least one GPU is available!\"\n",
    "distributed_state = PartialState()\n",
    "torch.cuda.set_device(device_id := distributed_state.local_process_index)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Unique Experiment ID & Log Directory\n",
    "exp_id = (\n",
    "    f\"{cfg.vla_path.split('/')[-1]}+{cfg.dataset_name}\"\n",
    "    f\"+b{cfg.batch_size * cfg.grad_accumulation_steps}\"\n",
    "    f\"+lr-{cfg.learning_rate}\"\n",
    ")\n",
    "if cfg.use_lora:\n",
    "    exp_id += f\"+lora-r{cfg.lora_rank}+dropout-{cfg.lora_dropout}\"\n",
    "if cfg.use_quantization:\n",
    "    exp_id += \"+q-4bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start =>> Build Directories\n",
    "run_dir, adapter_dir = cfg.run_root_dir / exp_id, cfg.adapter_tmp_dir / exp_id\n",
    "os.makedirs(run_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization Config =>> only if LoRA fine-tuning\n",
    "quantization_config = None\n",
    "if cfg.use_quantization:\n",
    "    assert cfg.use_lora, \"Quantized training only supported for LoRA fine-tuning!\"\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type=\"nf4\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8407f54d0a14246a5040d0d5bb83c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load OpenVLA Processor and Model using HF AutoClasses\n",
    "processor = AutoProcessor.from_pretrained(cfg.vla_path, trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    cfg.vla_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Placement =>> note that BitsAndBytes automatically handles for quantized training\n",
    "if cfg.use_quantization:\n",
    "    vla = prepare_model_for_kbit_training(vla)\n",
    "else:\n",
    "    vla = vla.to(device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 110,828,288 || all params: 7,652,065,472 || trainable%: 1.4483\n"
     ]
    }
   ],
   "source": [
    "# [LoRA] Wrap Model w/ PEFT `LoraConfig` =>> by default we set `target_modules=all-linear`\n",
    "if cfg.use_lora:\n",
    "    lora_config = LoraConfig(\n",
    "        r=cfg.lora_rank,\n",
    "        lora_alpha=min(cfg.lora_rank, 16),\n",
    "        lora_dropout=cfg.lora_dropout,\n",
    "        target_modules=\"all-linear\",\n",
    "        init_lora_weights=\"gaussian\",\n",
    "    )\n",
    "    vla = get_peft_model(vla, lora_config)\n",
    "    vla.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Optimizer =>> note that we default to a simple constant learning rate!\n",
    "trainable_params = [param for param in vla.parameters() if param.requires_grad]\n",
    "optimizer = AdamW(trainable_params, lr=cfg.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Action Tokenizer\n",
    "action_tokenizer = ActionTokenizer(processor.tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 22:37:44.401186: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n",
      "2024-07-08 22:37:46.057879: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">07/08 [22:37:46] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; <span style=\"font-weight: bold\">[</span>*<span style=\"font-weight: bold\">]</span> Loading existing dataset statistics from                       <a href=\"file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/utils/data_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">data_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/utils/data_utils.py#208\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">208</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         datasets/imperialcollege_sawyer_wrist_cam/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>/dataset_statistics_d28f <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         8c25e8e87a1c5284970cf0753d473e122d30af8453b59346d2e59432c7f5.json.      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m07/08 [22:37:46]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> \u001b[1m[\u001b[0m*\u001b[1m]\u001b[0m Loading existing dataset statistics from                       \u001b]8;id=980392;file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/utils/data_utils.py\u001b\\\u001b[2mdata_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=864008;file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/utils/data_utils.py#208\u001b\\\u001b[2m208\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         datasets/imperialcollege_sawyer_wrist_cam/\u001b[1;36m0.1\u001b[0m.\u001b[1;36m0\u001b[0m/dataset_statistics_d28f \u001b[2m                 \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m         8c25e8e87a1c5284970cf0753d473e122d30af8453b59346d2e59432c7f5.json.      \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 22:37:46.533527: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################################\n",
      "# Loading the following 1 datasets (incl. sampling weight):                         #\n",
      "# imperialcollege_sawyer_wrist_cam: ========================================1.000000 #\n",
      "######################################################################################\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">07/08 [22:37:47] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; <span style=\"font-weight: bold\">[</span>*<span style=\"font-weight: bold\">]</span> Threads per Dataset: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>                                          <a href=\"file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/dataset.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">dataset.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/dataset.py#531\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">531</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m07/08 [22:37:47]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> \u001b[1m[\u001b[0m*\u001b[1m]\u001b[0m Threads per Dataset: \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m                                          \u001b]8;id=986486;file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/dataset.py\u001b\\\u001b[2mdataset.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=839136;file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/dataset.py#531\u001b\\\u001b[2m531\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; <span style=\"font-weight: bold\">[</span>*<span style=\"font-weight: bold\">]</span> Reads per Dataset: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>                                            <a href=\"file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/dataset.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">dataset.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/dataset.py#532\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">532</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> \u001b[1m[\u001b[0m*\u001b[1m]\u001b[0m Reads per Dataset: \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m                                            \u001b]8;id=562475;file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/dataset.py\u001b\\\u001b[2mdataset.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=486317;file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/dataset.py#532\u001b\\\u001b[2m532\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; <span style=\"font-weight: bold\">[</span>*<span style=\"font-weight: bold\">]</span> Constructing datasets<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                          <a href=\"file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/dataset.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">dataset.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/dataset.py#535\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">535</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> \u001b[1m[\u001b[0m*\u001b[1m]\u001b[0m Constructing datasets\u001b[33m...\u001b[0m                                          \u001b]8;id=324027;file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/dataset.py\u001b\\\u001b[2mdataset.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=850497;file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/dataset.py#535\u001b\\\u001b[2m535\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 22:37:47.967546: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">07/08 [22:37:48] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; <span style=\"font-weight: bold\">[</span>*<span style=\"font-weight: bold\">]</span> Applying frame transforms on dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                           <a href=\"file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/dataset.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">dataset.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/dataset.py#575\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">575</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m07/08 [22:37:48]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> \u001b[1m[\u001b[0m*\u001b[1m]\u001b[0m Applying frame transforms on dataset\u001b[33m...\u001b[0m                           \u001b]8;id=291167;file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/dataset.py\u001b\\\u001b[2mdataset.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=328153;file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/dataset.py#575\u001b\\\u001b[2m575\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Fine-tuning Dataset =>> note that we use an RLDS-formatted dataset following Open X-Embodiment by default.\n",
    "#   =>> If you want to use a non-RLDS dataset (e.g., a standard PyTorch Dataset) see the following commented block.\n",
    "#   =>> Note that our training code does not loop over epochs because the RLDS loader does this implicitly; if using\n",
    "#       your own Dataset, make sure to add the appropriate logic to the training loop!\n",
    "#\n",
    "# ---\n",
    "# from prismatic.vla.datasets import DummyDataset\n",
    "#\n",
    "# vla_dataset = DummyDataset(\n",
    "#     action_tokenizer,\n",
    "#     processor.tokenizer,\n",
    "#     image_transform=processor.image_processor.apply_transform,\n",
    "#     prompt_builder_fn=PurePromptBuilder if \"v01\" not in cfg.vla_path else VicunaV15ChatPromptBuilder,\n",
    "# )\n",
    "# ---\n",
    "batch_transform = RLDSBatchTransform(\n",
    "    action_tokenizer,\n",
    "    processor.tokenizer,\n",
    "    image_transform=processor.image_processor.apply_transform,\n",
    "    prompt_builder_fn=PurePromptBuilder if \"v01\" not in cfg.vla_path else VicunaV15ChatPromptBuilder,\n",
    ")\n",
    "\n",
    "vla_dataset = RLDSDataset(\n",
    "    cfg.data_root_dir,\n",
    "    cfg.dataset_name,\n",
    "    batch_transform,\n",
    "    # resize_resolution=(vla.module.config.image_size, vla.module.config.image_size),\n",
    "    resize_resolution = (224, 224),\n",
    "    shuffle_buffer_size=cfg.shuffle_buffer_size,\n",
    "    image_aug=cfg.image_aug,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">07/08 [22:37:50] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; <span style=\"font-weight: bold\">[</span>*<span style=\"font-weight: bold\">]</span> Saved dataset statistics file at path                          <a href=\"file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/utils/data_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">data_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/utils/data_utils.py#289\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">289</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         runs/openvla-7b+imperialcollege_sawyer_wrist_cam+b2+lr-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2e-05</span>+lora-r32+d <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         ropout-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>+q-4bit/dataset_statistics.json                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m07/08 [22:37:50]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> \u001b[1m[\u001b[0m*\u001b[1m]\u001b[0m Saved dataset statistics file at path                          \u001b]8;id=478229;file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/utils/data_utils.py\u001b\\\u001b[2mdata_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=481982;file:///home/lawrence/VLA-RL/openvla/prismatic/vla/datasets/rlds/utils/data_utils.py#289\u001b\\\u001b[2m289\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         runs/openvla-7b+imperialcollege_sawyer_wrist_cam+b2+lr-\u001b[1;36m2e-05\u001b[0m+lora-r32+d \u001b[2m                 \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m         ropout-\u001b[1;36m0.0\u001b[0m+q-4bit/dataset_statistics.json                               \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [Important] Save Dataset Statistics =>> used to de-normalize actions for inference!\n",
    "if distributed_state.is_main_process:\n",
    "    save_dataset_statistics(vla_dataset.dataset_statistics, run_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Collator and DataLoader\n",
    "collator = PaddedCollatorForActionPrediction(\n",
    "    processor.tokenizer.model_max_length, processor.tokenizer.pad_token_id, padding_side=\"right\"\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    vla_dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    sampler=None,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0,  # Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla.train()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1720492670.370368   50945 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 224 } dim { size: 224 } dim { size: -7 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: \"CPU\" vendor: \"AuthenticAMD\" model: \"241\" frequency: 3592 num_cores: 16 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 524288 l3_cache_size: 16777216 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: -8 } dim { size: -9 } dim { size: -7 } } }\n",
      "W0000 00:00:1720492670.370750   50945 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 224 } dim { size: 224 } dim { size: -6 } } } inputs { dtype: DT_FLOAT shape { dim { size: -3 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -3 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: \"CPU\" vendor: \"AuthenticAMD\" model: \"241\" frequency: 3592 num_cores: 16 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 524288 l3_cache_size: 16777216 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -3 } dim { size: -10 } dim { size: -11 } dim { size: -6 } } }\n",
      "2024-07-08 22:37:50.581004: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "step_idx, batch = next(enumerate(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 27])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/lawrence/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    output: CausalLMOutputWithPast = vla(\n",
    "        input_ids=batch[\"input_ids\"].to(device_id),\n",
    "        attention_mask=batch[\"attention_mask\"].to(device_id),\n",
    "        pixel_values=batch[\"pixel_values\"].to(torch.bfloat16).to(device_id),\n",
    "        labels=batch[\"labels\"],\n",
    "    )\n",
    "    loss = output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward!\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vla.vision_backbone.featurizer.patch_embed.num_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 31958, 31898,\n",
       "         31819, 31857, 31852, 31876, 31744,     2],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100, 31869, 31895, 31840,\n",
       "         31855, 31916, 31821, 31872,     2,  -100]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"][:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Accuracy and L1 Loss for Logging\n",
    "# action_logits = output.logits[:, vla.module.vision_backbone.featurizer.patch_embed.num_patches : -1]\n",
    "action_logits = output.logits[:, vla.vision_backbone.featurizer.patch_embed.num_patches : -1]\n",
    "action_preds = action_logits.argmax(dim=2)\n",
    "action_gt = batch[\"labels\"][:, 1:].to(action_preds.device)\n",
    "mask = action_gt > action_tokenizer.action_token_begin_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 31958, 31898,\n",
       "         31819, 31857, 31852, 31876, 31744,     2],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100, 31869, 31895, 31840,\n",
       "         31855, 31916, 31821, 31872,     2,  -100]], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False,  True,  True,\n",
       "          True,  True,  True,  True,  True, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False,  True,  True,  True,\n",
       "          True,  True,  True,  True, False, False]], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 26, 32064])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Accuracy\n",
    "correct_preds = (action_preds == action_gt) & mask\n",
    "action_accuracy = correct_preds.sum().float() / mask.sum().float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute L1 Loss on Predicted (Continuous) Actions\n",
    "continuous_actions_pred = torch.tensor(\n",
    "    action_tokenizer.decode_token_ids_to_actions(action_preds[mask].cpu().numpy())\n",
    ")\n",
    "continuous_actions_gt = torch.tensor(\n",
    "    action_tokenizer.decode_token_ids_to_actions(action_gt[mask].cpu().numpy())\n",
    ")\n",
    "action_l1_loss = torch.nn.functional.l1_loss(continuous_actions_pred, continuous_actions_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer Step\n",
    "if (step_idx + 1) % cfg.grad_accumulation_steps == 0:\n",
    "    optimizer.step()\n",
    "    # progress.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
